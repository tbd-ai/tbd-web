<!DOCTYPE html>
<html lang="en">

<head>

	<title>Tools | TBD</title>
	
	<meta charset="utf-8">
	<meta name="viewpoint" content="width=device-width, initial-scale=1">
	<link rel="icon" href="./images/uoft.png">
	
	<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/css/bootstrap.min.css" 
		integrity="sha384-/Y6pD6FV/Vv2HJnA6t+vslU6fwYXjCFtcEpHbNJ0lyAFsXTsjBbfaDjzALeQsN6M" crossorigin="anonymous">
	<link rel="stylesheet" type="text/css" href="ecosystem.css">
	<link rel="stylesheet" type="text/css" href="tbd.css">
	<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">

	<script src="https://www.w3schools.com/lib/w3.js"></script>
	<script src="https://use.fontawesome.com/fd789ca0f7.js"></script>

</head>

<body>

	<div w3-include-html="./components/tbd-titlebar.html"></div>

	<main id="content" role="main" class="doc">
		<div class="container">
		    <br><br>
		    <h2>Tools</h2>
			<br>
			
			<div class="container">
				<p>As part of this project, we built tools to help profile memory usage and network usage. This is a brief introduction; for detailed usage, please refer to our <a href=https://github.com/tbd-ai/tbd-tools>tbd-tools<a> github repository.</p>
				<ul>
				<li class="toctree-l1"><a class="reference internal" href="tools-memory.html">Memory Profiler</a></li>
				<li class="toctree-l1"><a class="reference internal" href="tools-nwprof.html">Network Profiler</a></li>
				</ul>
				<div class="section" id="memory-profilers">
					<div>
						<h3>Memory Profiler</h3>
					</div>
					
					<div>
						<p>We built memory profilers for TensorFlow, MXNet and CNTK. 
						The goal of a memory profiler is to show the breakdown of the bulk of GPU 
						allocated memory involved in DNN training. We classify allocations 
						according to data structure, including:
						<ol>
						<li>Weights: weights, biases, etc.</li>
						<li>Gradients: calculated in the backward pass, often has equal size with Weights.</li>
						<li>Activations (or feature maps): intermediate results 
						generated in the forward pass and reused in the backward pass. This is the major consumer of GPU memory.</li>
						<li>Workspace: temporary space allocated for detailed implementations of operations 
						(e.g. matrix multiplications, convolutions).</li>
						<li>Dynamic: (explained below).</li>
						</ol>
						(Our MXNet memory profiler is available <a href="https://github.com/tbd-ai/tbd-tools/tree/master/MXNet-MemoryProfiler" target="_blank" class="link-style">here&nbsp;<span class="fa fa-link"></span></a>, we will gradually put others.)
						</p>

						<div>
							<h4>How the memory profiler works</h4>
						</div>
						<p>Our profiler works in two steps. The first step is to log each memory 
						allocation issued by the framework (which necessitates making changes 
						to framework code) and tag each allocation with the following 
						information at a minimum:</p>
						
						<ol>
						<li>The size of the allocated memory.</li>
						<li>The type of the associated data structure (as above).</li>
						</ol>

						The second step is to process the log using a parser program. The whole profiling stack is shown following:

						<br>
						<br>
						<img class="col-md-6 col-xs-12 margin-bottom-20 max-width-100 width-900" 
							src="./images/sigmetrics18/memory-profiler.png">
						<br>

						<p>Generally, each framework employs a wrapper around low-level 
						memory allocation calls, such as cudaMalloc(...). Unfortunately, the
						latter information is usually not directly accessible from the wrapper.
						To obtain this information, we must trace the call stack up to where 
						the type of the data structure is known and pass that information back 
						down to the wrapper. The definitions of all functions on the stack in 
						between must be altered.</p>

						<p>Unfortunately, due to the complexity of the code, it is often very 
						difficult to trace the call stack and find the right place to pass 
						data structure information. The allocation wrapper function may be 
						called in the initialization function of a tensor, which is invoked 
						in an enormous number of places, and only a small portion of 
						initializations actually trigger memory allocation. Through research 
						and careful analysis on the framework codebases, and with additional 
						hacking skills, we managed to attach the correct data structure 
						information to most of the memory allocations, though there are still 
						potentially unclassified allocations that have not been accounted for 
						yet. This issue can be incrementally fixed as we test the memory 
						profiler with more applications.</p>

						<p>The core code of TensorFlow, MXNet and CNTK are all written in 
						C/C++. Since all of them support bindings with python, memory 
						allocations may be triggered implicitly from python (especially for MXNet). To capture <i>all</i> 
						these allocations and tag them with the correct data structure 
						information likely requires a complete restructure of the framework 
						code. Our memory profilers are not yet able to classify them. 
						As we noticed, most of them are generated during the training epochs, while
						weights, gradients, activations are mostly initialized before training starts, 
						We assign these allocations with a new type called "Dynamic".</p>

						<p>After obtaining the memory allocation log, it can then be parsed 
						by a simple python program that outputs the allocation breakdown results. 
						The python program can easily tell the data structure if the proper keywords are attached. We aim to 
						open source our modified version of the frameworks with memory 
						profiling capability, alongside our python parsers soon.</p>

						<div>
							<h4>Example results</h4>
						</div>
						<p>Below are two examples from Inception and Deep Speech 2 that show the results of our memory profiling. 
						<br>
						<br>
						<img class="col-md-6 col-xs-12 margin-bottom-20 max-width-100 width-500" 
							src="./images/sigmetrics18/memory_inception.png">
						<img class="col-md-6 col-xs-12 margin-bottom-20 max-width-100 width-500" 
							src="./images/sigmetrics18/memory_DS2.png">
						<br>
						Notice that the memory consumption calculated by our memory profiler is 
						less then the real memory consumption (one can check using nvidia-smi). The major
						reason is memory paging. Since the page size is 2 MB, each memory
						allocation that doesn't fully fit in a page contributes to inflating the actual memory consumption. The waste accumulated can
						make the actual memory consumption hundreds of MB greater than the calculated memory consumption.

						<div>
							<h4>Current progress</h4>
						</div>
						<p>We have been collaborating closely with MXNet people to merge the memory profiler into the main branch. Please visit <a href="https://github.com/apache/incubator-mxnet/pull/14973">this page</a> for the current status of the merging process.
					</div>
					<br><br>
				</div>
			</div>
		</div>
        <br><br>
	</main>
	<script>w3.includeHTML();</script>

	<script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" 
		integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" 
		crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.11.0/umd/popper.min.js" 
		integrity="sha384-b/U6ypiBEHpOf/4+1nzFpr53nxSS+GLCkfwBdFNTxtclqqenISfwAzpKaMNFNmj4" 
		crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/js/bootstrap.min.js" 
		integrity="sha384-h0AbiXch4ZDo7tp9hKZ4TsHbi047NrKGLO3SEJAg45jXxnGIfYzk4Si90RDIqNm1" 
		crossorigin="anonymous"></script>

</body>
</html>
